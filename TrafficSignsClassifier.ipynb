{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as a\n",
    "import efficientnet_pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from albumentations.pytorch import ToTensor\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import SGD \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versions of libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './traffic-signs-data/'\n",
    "\n",
    "with open(os.path.join(data_folder,'train.p'), mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(os.path.join(data_folder,'valid.p'), mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(os.path.join(data_folder,'test.p'), mode='rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.keys(), valid.keys(), test.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, coords_train, sizes_train = train['features'], train['labels'], train['coords'], train['sizes']\n",
    "X_valid, y_valid, coords_valid, sizes_valid = valid['features'], valid['labels'], valid['coords'], valid['sizes']\n",
    "X_test, y_test, coords_test, sizes_test = test['features'], test['labels'], test['coords'], test['sizes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples =\", X_train.shape[0])\n",
    "print(\"Number of validation examples =\", X_valid.shape[0])\n",
    "print(\"Number of testing examples =\", X_test.shape[0])\n",
    "\n",
    "height, width, channels = X_train.shape[1:]\n",
    "print(f\"Image data shape = {width}x{height}, channels={channels}, dtype={X_train.dtype}\")\n",
    "n_classes = len(set(train['labels']))\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_image(img, text):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.title(text)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "\n",
    "def preview_image_from_dataset(dataset, index):\n",
    "    preview_image(dataset[idx], f\"Index: {index}\")\n",
    "\n",
    "for idx in [123,234,456,678]:\n",
    "    preview_image_from_dataset(X_train, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check histogram of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.title(\"Histogram of classes in dataset\")\n",
    "\n",
    "plt.hist(y_train,bins = n_classes, alpha=0.2, label = 'train')\n",
    "plt.hist(y_test,bins = n_classes, alpha=0.2, label='test')\n",
    "plt.hist(y_valid,bins = n_classes, alpha=0.2, label='valid')\n",
    "\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity of distribution of number of items per class in train/test/valid seems to be good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmentation = a.Compose([\n",
    "    a.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "test_augmentation = a.Compose([\n",
    "    a.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),\n",
    "        std=(0.5, 0.5, 0.5)),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignsDataset(Dataset):\n",
    "    def __init__(self, features, labels, augmentation):\n",
    "        self.features = features.copy()\n",
    "        self.labels  = labels.copy()\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, index, noaug=False):\n",
    "        img = self.features[index]\n",
    "        if noaug:\n",
    "            img = img\n",
    "        else:\n",
    "            img = self.augmentation(image=img)['image']\n",
    "        \n",
    "        label = int(self.labels[index])\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add comment on preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrafficSignsDataset(X_train, y_train, train_augmentation)\n",
    "test_dataset = TrafficSignsDataset(X_test, y_test, test_augmentation)\n",
    "valid_dataset = TrafficSignsDataset(X_valid, y_valid, test_augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if datasets work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_dataset.__getitem__(123, noaug=True)\n",
    "preview_image(img, f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of experimenting with own model using one that has performance of ResNet-50 while staying extremely small:   \n",
    "Model name: **EfficientNet, version \"B0\"**\n",
    "\n",
    "Link to paper: https://arxiv.org/pdf/1905.11946.pdf  \n",
    "Using implementation from: https://github.com/lukemelas/EfficientNet-PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efficientnet_pytorch.EfficientNet.from_name('efficientnet-b0', override_params={'num_classes': n_classes})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if sample outut has proper shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.zeros((1,3,32,32))\n",
    "model.eval()\n",
    "model(dummy).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsAggregator:\n",
    "    def __init__(self):\n",
    "        self.epoch_index = -1\n",
    "        self.loss_history = {}\n",
    "        self.accuracy_history = {}\n",
    "    \n",
    "    def epoch_start(self):\n",
    "        self.epoch_index += 1\n",
    "        \n",
    "    def __add_loss(self, loss):\n",
    "        if self.epoch_index not in self.loss_history:\n",
    "            self.loss_history[self.epoch_index] = []\n",
    "        self.loss_history[self.epoch_index].append(loss)\n",
    "        \n",
    "    def add(self, loss, pred:np.ndarray, gt:np.ndarray):\n",
    "        self.__add_loss(loss)\n",
    "    \n",
    "    def get_mean_loss(self, samples=None):\n",
    "        if samples is None:\n",
    "            return np.mean(self.loss_history[self.epoch_index])\n",
    "        return np.mean(self.loss_history[self.epoch_index][-samples:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, loader, aggregator, criterion, device):\n",
    "    aggregator.epoch_start()\n",
    "    net.train()\n",
    "    \n",
    "    with tqdm(desc=\"Train epoch progress\", total=len(loader)) as p:\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            aggregator.add(float(loss.item()), outputs.cpu().detach().numpy(), labels.cpu().detach().numpy())\n",
    "            batch_loss = aggregator.get_mean_loss(inputs.shape[0])\n",
    "            total_loss = aggregator.get_mean_loss()\n",
    "            p.desc = f\"Loss:{batch_loss:0.2f} ({total_loss:0.2f})\"\n",
    "            p.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, loader, aggregator, criterion, device):\n",
    "    net.eval()\n",
    "    aggregator.epoch_start()\n",
    "    with torch.no_grad():\n",
    "        with tqdm(desc=\"Eval progress\", total=len(loader)) as p:\n",
    "            for inputs, labels in loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                aggregator.add(float(loss.item()), outputs.cpu().detach().numpy(), labels.cpu().detach().numpy())\n",
    "                batch_loss = aggregator.get_mean_loss(inputs.shape[0])\n",
    "                total_loss = aggregator.get_mean_loss()\n",
    "                p.desc = f\"Loss:{batch_loss:0.2f} ({total_loss:0.2f})\"\n",
    "                p.update(1)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,batch_size=4, shuffle=True)\n",
    "post_train_dataloader = DataLoader(train_dataset,batch_size=4, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=4, shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_train_metrics = MetricsAggregator()\n",
    "train_metrics = MetricsAggregator()\n",
    "val_metrics = MetricsAggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=4e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=3, verbose=True, min_lr=1e-8, factor=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(desc='Training', total=epochs) as p:\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        train(model, train_dataloader, running_train_metrics, criterion, device)\n",
    "        test(model, train_dataloader, train_metrics, criterion, device)\n",
    "        test(model, valid_dataloader, val_metrics, criterion, device)\n",
    "        \n",
    "        p.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External sample traffic signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
